{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3101f835",
   "metadata": {},
   "source": [
    "# 作业1，RAG agent\n",
    "通过LLAMA-CPP 模型实现一个多Agent问答系统"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84235ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install --no-cache-dir llama-cpp-python==0.3.4 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
    "!python3 -m pip install googlesearch-python bs4 charset-normalizer requests-html lxml_html_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c811f61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "if not Path('./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf').exists():\n",
    "    !wget https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\n",
    "if not Path('./public.txt').exists():\n",
    "    !wget https://www.csie.ntu.edu.tw/~ulin/public.txt\n",
    "if not Path('./private.txt').exists():\n",
    "    !wget https://www.csie.ntu.edu.tw/~ulin/private.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014176b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# Load the model onto GPU\n",
    "llama3 = Llama(\n",
    "    \"./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\",\n",
    "    verbose=False,\n",
    "    n_gpu_layers=-1,\n",
    "    n_ctx=16384,    # This argument is how many tokens the model can take. The longer the better, but it will consume more memory. 16384 is a proper value for a GPU with 16GB VRAM.\n",
    ")\n",
    "\n",
    "def generate_response(_model: Llama, _messages: str) -> str:\n",
    "    '''\n",
    "    This function will inference the model with given messages.\n",
    "    '''\n",
    "    _output = _model.create_chat_completion(\n",
    "        _messages,\n",
    "        stop=[\"<|eot_id|>\", \"<|end_of_text|>\"],\n",
    "        max_tokens=512,    # This argument is how many tokens the model can generate, you can change it and observe the differences.\n",
    "        temperature=0,      # This argument is the randomness of the model. 0 means no randomness. You will get the same result with the same input every time. You can try to set it to different values.\n",
    "        repeat_penalty=2.0,\n",
    "    )[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return _output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc05231e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from googlesearch import search as _search\n",
    "from bs4 import BeautifulSoup\n",
    "from charset_normalizer import detect\n",
    "import asyncio\n",
    "from requests_html import AsyncHTMLSession\n",
    "import urllib3\n",
    "urllib3.disable_warnings()\n",
    "\n",
    "async def worker(s:AsyncHTMLSession, url:str):\n",
    "    try:\n",
    "        header_response = await asyncio.wait_for(s.head(url, verify=False), timeout=10)\n",
    "        if 'text/html' not in header_response.headers.get('Content-Type', ''):\n",
    "            return None\n",
    "        r = await asyncio.wait_for(s.get(url, verify=False), timeout=10)\n",
    "        return r.text\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "async def get_htmls(urls):\n",
    "    session = AsyncHTMLSession()\n",
    "    tasks = (worker(session, url) for url in urls)\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "async def search(keyword: str, n_results: int=3) -> List[str]:\n",
    "    '''\n",
    "    This function will search the keyword and return the text content in the first n_results web pages.\n",
    "\n",
    "    Warning: You may suffer from HTTP 429 errors if you search too many times in a period of time. This is unavoidable and you should take your own risk if you want to try search more results at once.\n",
    "    The rate limit is not explicitly announced by Google, hence there's not much we can do except for changing the IP or wait until Google unban you (we don't know how long the penalty will last either).\n",
    "    '''\n",
    "    keyword = keyword[:100]\n",
    "    # First, search the keyword and get the results. Also, get 2 times more results in case some of them are invalid.\n",
    "    results = list(_search(keyword, n_results * 2, lang=\"zh\", unique=True))\n",
    "    # Then, get the HTML from the results. Also, the helper function will filter out the non-HTML urls.\n",
    "    results = await get_htmls(results)\n",
    "    # Filter out the None values.\n",
    "    results = [x for x in results if x is not None]\n",
    "    # Parse the HTML.\n",
    "    results = [BeautifulSoup(x, 'html.parser') for x in results]\n",
    "    # Get the text from the HTML and remove the spaces. Also, filter out the non-utf-8 encoding.\n",
    "    results = [''.join(x.get_text().split()) for x in results if detect(x.encode()).get('encoding') == 'utf-8']\n",
    "    # Return the first n results.\n",
    "    return results[:n_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1d5f7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "泰勒絲（Taylor Swift）是一位美國歌手、詞曲作家和製作人。她出生於1989年，來自田納西州。她的音樂風格從鄉村搖滾開始逐漸轉變為流行電音。\n",
      "\n",
      "她早期的作品如《泰勒絲第一輯》、《愛情故事第二章：睡美人的秘密》，獲得了廣泛認可和獎項，包括多個告示牌音樂大奖。後來，她推出了更具商業成功性的專辑，如 《1989》（2014）、_reputation（《名聲_(泰勒絲专輯)》） （ 20 ） 和 _Lover(2020)，並且在全球取得了巨大的影響力。\n",
      "\n",
      "她以她的歌曲如 \"Shake It Off\"、\"_Blank Space_\"和 \"_Bad Blood_\",以及與其他藝人合作的作品，如 《Look What You Made Me Do》（2017）而聞名。泰勒絲還是知識產權運動的一部分，對於音樂創作者在數字時代獲得公平報酬有所關注。\n",
      "\n",
      "她被譽為當代最成功和影響力最大的人物之一，並且她的歌曲經常成為流行文化的話題。\n"
     ]
    }
   ],
   "source": [
    "# You can try out different questions here.\n",
    "test_question='請問誰是 Taylor Swift？'\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"你是 LLaMA-3.1-8B，是用來回答問題的 AI。使用中文時只會使用繁體中文來回問題。\"},    # System prompt\n",
    "    {\"role\": \"user\", \"content\": test_question}, # User prompt\n",
    "]\n",
    "\n",
    "print(generate_response(llama3, messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc985a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMAgent():\n",
    "    def __init__(self, role_description: str, task_description: str, llm:str=\"bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\"):\n",
    "        self.role_description = role_description   # Role means who this agent should act like. e.g. the history expert, the manager......\n",
    "        self.task_description = task_description    # Task description instructs what task should this agent solve.\n",
    "        self.llm = llm  # LLM indicates which LLM backend this agent is using.\n",
    "    def inference(self, message:str) -> str:\n",
    "        if self.llm == 'bartowski/Meta-Llama-3.1-8B-Instruct-GGUF': # If using the default one.\n",
    "            # TODO: Design the system prompt and user prompt here.\n",
    "            # Format the messsages first.\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": f\"{self.role_description}\"},  # Hint: you may want the agents to speak Traditional Chinese only.\n",
    "                {\"role\": \"user\", \"content\": self.task_description.format(message)}, # Hint: you may want the agents to clearly distinguish the task descriptions and the user messages. A proper seperation text rather than a simple line break is recommended.\n",
    "            ]\n",
    "            return generate_response(llama3, messages)\n",
    "        else:\n",
    "            # TODO: If you want to use LLMs other than the given one, please implement the inference part on your own.\n",
    "            return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060fc1e1",
   "metadata": {},
   "source": [
    "随便写的prompt，可能不太好用，仅供参考。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26ae2078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This agent may help you filter out the irrelevant parts in question descriptions.\n",
    "question_extraction_agent = LLMAgent(\n",
    "    role_description=\"你擅长总结问题。你会在用户输入中找到用户提出的问题，确定其中的关键信息，并且用最精简的语言提取出用户提出的问题。输出内容只有总结好的问题，不要回答问题，不要有多余内容。\",\n",
    "    task_description=\"以下是用户输入:\\\"{}\\\"，根据用户的输入，总结成一个精炼的问题。\",\n",
    ")\n",
    "\n",
    "# This agent may help you extract the keywords in a question so that the search tool can find more accurate results.\n",
    "keyword_extraction_agent = LLMAgent(\n",
    "    role_description=\"你是Google的熟练使用者。根据用户的问题，你要为其设计出包含关键信息，且最适合用于互联网搜索的关键字，数量为2-4个，关键字之间以空格相连。不要回答问题，不要输出额外内容\",\n",
    "    task_description=\"以下是用户问题:\\\"{}\\\"，根据问题总结出2-4个最适合其用于Google搜索的关键词。\",\n",
    ")\n",
    "\n",
    "# This agent is the core component that answers the question.\n",
    "qa_agent = LLMAgent(\n",
    "    role_description=\"你是 LLaMA-3.1-8B，是用來回答問題的AI。会根据用户的提问和网络信息回答，你的输出只包含对应问题的答案，回答长度为一个词，或者最长为一句话\",\n",
    "    task_description=\"以下是用户的原始问题，经过总结的问题，和根据总结的问题搜索到的网络信息:\\\"{}\\\"。用户原始问题的最终答案是?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11aef989",
   "metadata": {},
   "source": [
    "按照以下形式做一个agent pipeline\n",
    "- RAG with agents (strong baseline)\n",
    "\n",
    "    ![](https://www.csie.ntu.edu.tw/~ulin/rag_agent.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4a240ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def pipeline(question: str) -> str:\n",
    "    ex_question = question_extraction_agent.inference(question)\n",
    "    print(f\"question_extraction_agent: {ex_question}\")\n",
    "    keywords = keyword_extraction_agent.inference(ex_question)\n",
    "    print(f\"keyword_extraction_agent: {keywords}\")\n",
    "    search_results = await search(keywords)\n",
    "    search_results = [i[:4000] for i in search_results]\n",
    "    return qa_agent.inference(f\"原始问题{question},根据原始提取的问题：{ex_question},搜索结果：{''.join(search_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e03c013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 校歌為學校（包括小學、中學、大學等）宣告或者規定的代表該校的歌曲。用於體現該校的治學理念、辦學理想等學校文化。「虎山雄風飛揚」是哪間學校的校歌歌詞？\n",
      "question_extraction_agent: \"虎山雄風飛揚是哪間學校的校歌?\"\n",
      "keyword_extraction_agent: 虎山雄風飛揚 校歌\n",
      "LLM:光華國小\n",
      "2 2025年初，NCC透過行政命令，規定民眾如果透過境外郵購無線鍵盤、滑鼠、藍芽耳機..等自用產品回台，每案一律加收審查費多少錢？\n",
      "question_extraction_agent: 2025年初，境外郵購自用產品（如無線鍑盤、滑鼠等）回台需繳交多少審查費？\n",
      "keyword_extraction_agent: 郵購自用產品審查費台湾\n",
      "LLM:750\n",
      "3 第一代 iPhone 是由哪位蘋果 CEO 發表？\n",
      "question_extraction_agent: 第一代 iPhone 由哪位苹果 CEO 发表？\n",
      "keyword_extraction_agent: 史蒂夫·乔布斯\n",
      "LLM:史蒂夫·乔布斯\n",
      "4 台灣大學進階英文免修申請規定中，托福網路測驗 TOEFL iBT 要達到多少分才能申請？\n",
      "question_extraction_agent: 托福網路測驗 TOEFL iBT 達到多少分才能申請台灣大學進階英文免修？\n",
      "keyword_extraction_agent: 托福網路測驗 TOEFL iBT 分數 台灣大學英文免修\n",
      "LLM:92\n",
      "5 Rugby Union 中觸地 try 可得幾分？\n",
      "question_extraction_agent: Rugby Union 中觸地 try 可得幾分？\n",
      "keyword_extraction_agent: Rugby Union 觸地 try 分数\n",
      "LLM:觸地 try 可得 5 分。\n",
      "6 卑南族是位在臺東平原的一個原住民族，以驍勇善戰、擅長巫術聞名，曾經統治整個臺東平原。相傳卑南族的祖先發源自 ruvuwa'an，該地位於現今的哪個行政區劃？\n",
      "question_extraction_agent: 卑南族的祖先發源地在哪個行政區劃？\n",
      "keyword_extraction_agent: 台東縣\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Fill in your student ID first.\n",
    "STUDENT_ID = \"a\"\n",
    "\n",
    "STUDENT_ID = STUDENT_ID.lower()\n",
    "with open('./public.txt','r') as input_f:\n",
    "    questions = input_f.readlines()\n",
    "    questions = [l.strip().split(',')[0] for l in questions]\n",
    "    for id, question in enumerate(questions, 1):\n",
    "        if Path(f\"./{STUDENT_ID}_{id}.txt\").exists():\n",
    "            continue\n",
    "        print(id,question)\n",
    "        answer = await pipeline(question)\n",
    "        answer = answer.replace('\\n',' ')\n",
    "        print(f\"LLM:{answer}\")\n",
    "        # with open(f'./{STUDENT_ID}_{id}.txt', 'w') as output_f:\n",
    "        #     print(answer, file=output_f)\n",
    "\n",
    "with open('./private.txt', 'r') as input_f:\n",
    "    questions = input_f.readlines()\n",
    "    for id, question in enumerate(questions, 31):\n",
    "        if Path(f\"./{STUDENT_ID}_{id}.txt\").exists():\n",
    "            continue\n",
    "        print(id,question)\n",
    "        answer = await pipeline(question)\n",
    "        answer = answer.replace('\\n',' ')\n",
    "        print(f\"LLM:{answer}\")\n",
    "        # with open(f'./{STUDENT_ID}_{id}.txt', 'a') as output_f:\n",
    "        #     print(answer, file=output_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fafd0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
